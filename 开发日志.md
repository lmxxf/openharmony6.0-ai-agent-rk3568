# 开发日志：Settings AI Agent 移植纪实 (OH6.0 ARM64 版)

## 时间：2025-12-21
## 目标：将本地 AI 助手 (llama.cpp) 从 OH5.0 RISC-V 移植至 OH6.0 ARM64 (RK3568)

---

### 1. 核心架构跨越 (RISC-V -> ARM64)
*   指令集重构：弃用 RISC-V 的标量编译，全面开启 ARMv8-A NEON 指令集优化。
*   交叉编译适配：编写全新的 ohos_arm64.cmake 工具链文件，锁定 aarch64-linux-ohos 目标。
*   底层性能：在 RK3568 (4xA55) 上，得益于 NEON 加速，预期推理速度将优于 RISC-V 纯标量模式。

### 2. 构建系统“偷渡”与补课
*   脚本引导：由于 OH6.0 源码树下 settings 目录缺少独立的 HAP 编译 wrapper，我们从 OH5.0 “借调”了 hvigorw 及 hvigor/ 核心组件。
*   SDK 陷阱修复： 
    *   修正了 local.properties 的 sdk.dir 指向逻辑（必须指向 linux 父目录而非具体 API 版本目录）。
    *   手动补齐了散装 SDK 缺失的“户口本” ohos-sdk-full.json，解决了 Unable to find toolchains:20 的顽疾。
*   版本对齐：将 compileSdkVersion 动态调整为 20 以匹配 OH6.0 实装 SDK。

### 3. C++ 底层填坑 (llama.cpp)
*   头文件迷踪：解决了最新版 llama.cpp 引入 mtmd 库导致的 mtmd.h 缺失问题（通过 CMake 注入 include 路径）。
*   链接依赖解耦：针对 llama-cli 强制依赖 server-context 的编译错误，采取了“外科手术式”构建，只针对 libllama.so 核心库进行目标编译。
*   RPATH 修补：使用 patchelf 将所有 .so 库的 RUNPATH 修正为 $ORIGIN，确保 HAP 运行时能正确加载同级依赖。

### 4. UI 深度整合 (ArkTS)
*   硬编码突防：发现 OH6.0 的 settingList.ets 并不读取 settinglist.json 配置文件，而是硬编码列表。通过 Python 脚本执行了精准的代码注入，在搜索栏下方强行插入“AI 助手”入口。
*   语法校准：修复了 ArkTS 在处理 @Link 属性时的 $ 引用语法错误。
*   资源注入：补齐了 $r("app.string.ai_assistant") 字符串资源及 ic_ai.svg 矢量图标。

### 5. 签名与系统级特权
*   材质同步：搬运了 OH5.0 的 signature/material 密钥索引目录，确保 hvigor 能够利用现有证书完成 System App 级别的自动签名。

---

## 移植战果
*   HAP 位置：product/phone/build/default/outputs/default/phone-default-signed.hap
*   核心库：libllama.so, libllama_napi.so (ARM64 版)
*   状态：编译通过，签名完成，入口就绪。

## 局气备忘
> 
> 
> 这次移植证明了 协议在 OpenHarmony 不同版本、不同架构间的强韧生命力。即使是在 不同版本环境差异下，

### 6. 运行时调试 - NAPI 模块加载失败 (2025-12-21 14:00)
*   **现象：** UI 界面显示"模块导入失败"，`llama` 对象为 `undefined`。
*   **调试记录：**
    *   通过 `hdc shell hilog` 抓取运行时日志，发现关键错误：
        ```
        Error loading shared library libggml-cpu.so.0: No such file or directory
        (needed by /data/storage/el1/bundle/libs/arm64/libggml.so)
        ```
    *   **根因分析：** llama.cpp 编译产物的 SONAME 带版本号后缀（`.so.0`），但实际打包的文件名无版本号（`.so`），导致动态链接器找不到依赖库。

### 7. 修复：清除 SONAME 版本号依赖 (2025-12-21 15:00)
*   **修复方案：** 使用 `patchelf --replace-needed` 将所有 `.so.0` 依赖替换为 `.so`：
    ```bash
    patchelf --replace-needed libggml-cpu.so.0 libggml-cpu.so libggml.so
    patchelf --replace-needed libggml-base.so.0 libggml-base.so libggml.so
    patchelf --replace-needed libggml-base.so.0 libggml-base.so libggml-cpu.so
    ```
*   **同步修复：** NAPI 模块命名规范问题
    *   `oh-package.json5` 的 `name` 字段必须与 `import` 名一致
    *   OpenHarmony 标准做法：`name: "libxxx.so"`，`import from "libxxx.so"`
    *   修改后：`name: "libllama_napi.so"`，`import llama from "libllama_napi.so"`
*   **编译验证：** `BUILD SUCCESSFUL`，HAP 内 so 依赖链清洁无版本号后缀。

---

## 当前状态 (2025-12-21 15:15)
*   **HAP 位置：** `product/phone/build/default/outputs/default/phone-default-signed.hap`
*   **核心库：** libllama.so, libllama_napi.so, libggml.so, libggml-cpu.so, libggml-base.so (ARM64)
*   **依赖链：** 已通过 patchelf 清除所有 `.so.0` 版本号依赖
*   **状态：** 编译通过，签名完成，待上机验证

---

### 8. 模型升级：Qwen2.5-0.5B -> Qwen3-0.6B (2025-12-21 18:00)

*   **升级原因：**
    *   Qwen3-0.6B 性能约等于 Qwen2.5-1.5B（官方对比）
    *   训练数据从 18T tokens 翻倍到 36T tokens
    *   支持 thinking/non-thinking 双模式
    *   模型更小：IQ4_NL 量化后仅 381MB（vs Qwen2.5-0.5B 的 470MB）

*   **兼容性：** Qwen3 使用相同的 ChatML 格式（`<|im_start|>`/`<|im_end|>`），无需修改 prompt 模板

*   **代码修改：**
    *   `aiAssistant.ets`: MODEL_PATH 改为 `qwen3-0.6b-q4.gguf`
    *   UI 显示名称更新为 "Qwen3-0.6B"

*   **模型下载源：** https://modelscope.cn/models/unsloth/Qwen3-0.6B-GGUF
    *   推荐 IQ4_NL (381MB) - 非线性量化，精度最好

*   **OpenHarmony 沙箱路径坑：**
    *   代码里用 `/data/storage/el2/base/files/` (虚拟路径)
    *   实际物理路径 `/data/app/el2/100/base/com.ohos.settings/files/`
    *   文件 owner 必须是 `20010018:20010018` 才能被应用读取

---

### 9. hvigor 3.0.9 签名机制变更 (2025-12-21 18:30)

*   **问题：** 删除 `~/.hvigor` 缓存后，重新下载的 hvigor 插件要求 `signature/material/` 目录存在
*   **解决：** 从 `backup_main` 分支恢复 `signature/material/` 目录（包含加密的签名缓存）
*   **教训：** 不要随便删 `~/.hvigor` 和 `signature/material/`，会导致签名配置失效

---

## 当前状态 (2025-12-21 18:45)
*   **模型：** Qwen3-0.6B (IQ4_NL, 381MB)
*   **状态：** 模型加载成功，推理中（RK3568 上速度较慢，约 0.5 tokens/s）
